{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softlab-data-collection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishi255/softlab-a3/blob/main/softlab_data_collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7iigpLCxph8"
      },
      "source": [
        "%pip install -qU omdb\n",
        "%pip install -qU torch\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import omdb\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import traceback\n",
        "import torch\n",
        "\n",
        "print(\"All installed and imported!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDPSVCH00BfB"
      },
      "source": [
        "# Data Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvQPe3qfvcVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3352b46-566f-448b-b942-1f1978f5b815"
      },
      "source": [
        "# get screenplay of all movies returned in the search\n",
        "def scrape(name_with_dash_list):\n",
        "    screenplays = []\n",
        "    print(name_with_dash_list)\n",
        "    \n",
        "    # for movie_id, movie_title in movies_list:\n",
        "    for movie_name_with_dash in name_with_dash_list: \n",
        "        print(\"For movie\", movie_name_with_dash)\n",
        "        # name_with_dash = \"-\".join(movie_title.split())\n",
        "        screenplay_url = f\"https://imsdb.com/scripts/{movie_name_with_dash}.html\"\n",
        "        print(\"Attempting to get the screenplay from url\", screenplay_url)\n",
        "        # XPATH = /html/body/table[2]/tbody/tr/td[3]/table/tbody/tr/td/pre/pre\n",
        "        response = requests.request(method=\"get\", url=screenplay_url)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        try:\n",
        "            screenplay_text = soup.find('td', class_=\"scrtext\").pre.pre\n",
        "            # print(\"Here\", type(screenplay_text), str(screenplay_text)[:300])\n",
        "            try:\n",
        "                screenplay_text = screenplay_text.text\n",
        "            except:\n",
        "                screenplay_text = \"\"\n",
        "        except:\n",
        "            print(f\"Movie {movie_name_with_dash} not found!\")\n",
        "            traceback.print_exc()\n",
        "            # return None\n",
        "        else:\n",
        "            screenplays.append(screenplay_text)\n",
        "            print(f\"*** {movie_name_with_dash} done!\")    \n",
        "\n",
        "    with open(\"merged.txt\", \"w+\") as f:\n",
        "        f.writelines(screenplays)\n",
        "\n",
        "scrape([\"Jurassic-Park\", \"Jurassic-Park-III\", \"Jurassic-Park-The-Lost-World\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Jurassic-Park', 'Jurassic-Park-III', 'Jurassic-Park-The-Lost-World']\n",
            "For movie Jurassic-Park\n",
            "Attempting to get the screenplay from url https://imsdb.com/scripts/Jurassic-Park.html\n",
            "*** Jurassic-Park done!\n",
            "For movie Jurassic-Park-III\n",
            "Attempting to get the screenplay from url https://imsdb.com/scripts/Jurassic-Park-III.html\n",
            "*** Jurassic-Park-III done!\n",
            "For movie Jurassic-Park-The-Lost-World\n",
            "Attempting to get the screenplay from url https://imsdb.com/scripts/Jurassic-Park-The-Lost-World.html\n",
            "*** Jurassic-Park-The-Lost-World done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdPKXIFB0Ecy"
      },
      "source": [
        "# Data Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqDp139ayRG6"
      },
      "source": [
        "# !cat JP1.txt JP2.txt JP3.txt > merged.txt\n",
        "# !wc -l JP1.txt\n",
        "# !wc -l JP2.txt\n",
        "# !wc -l JP3.txt\n",
        "\n",
        "!wc -l screenplays.txt\n",
        "# !wc -c merged.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMnudKtgz2ha"
      },
      "source": [
        "# Actual Training and Predicting stuff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt0_g79h1NcC"
      },
      "source": [
        "%pip install transformers/\n",
        "%pip install -r transformers/examples/language-modeling/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjasosq2z1o0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbca21fc-1bfd-4bb1-be88-da8655a2c733"
      },
      "source": [
        "!ls\n",
        "!pwd\n",
        "!python transformers/examples/language-modeling/run_clm.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --do_train \\\n",
        "    --train_file=/content/merged.txt \\\n",
        "    --overwrite_output_dir\\\n",
        "    --block_size=200\\\n",
        "    --per_device_train_batch_size=1\\\n",
        "    --save_steps 5000\\\n",
        "    --num_train_epochs=2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "merged.txt  sample_data  transformers\n",
            "/content\n",
            "2021-04-12 07:31:17.695001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/12/2021 07:31:20 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/12/2021 07:31:20 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr12_07-31-20_f40b5459fdea, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
            "04/12/2021 07:31:21 - WARNING - datasets.builder -   Using custom data configuration default-ea878203f6435c11\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-ea878203f6435c11/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-ea878203f6435c11/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1394] 2021-04-12 07:31:21,966 >> https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfyawtexk\n",
            "Downloading: 100% 718/718 [00:00<00:00, 576kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-12 07:31:22,526 >> storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|file_utils.py:1401] 2021-04-12 07:31:22,527 >> creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:490] 2021-04-12 07:31:22,528 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:526] 2021-04-12 07:31:22,528 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:490] 2021-04-12 07:31:23,089 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:526] 2021-04-12 07:31:23,090 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-12 07:31:23,656 >> https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0t_3x9xg\n",
            "Downloading: 100% 1.04M/1.04M [00:01<00:00, 959kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-12 07:31:25,310 >> storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1401] 2021-04-12 07:31:25,310 >> creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1394] 2021-04-12 07:31:25,864 >> https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8h335krb\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 514kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-12 07:31:27,305 >> storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1401] 2021-04-12 07:31:27,306 >> creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1394] 2021-04-12 07:31:27,872 >> https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpytnbvz3s\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 1.26MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-12 07:31:29,510 >> storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1401] 2021-04-12 07:31:29,510 >> creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-12 07:31:31,191 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-12 07:31:31,191 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-12 07:31:31,191 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-12 07:31:31,192 >> loading file https://huggingface.co/gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-12 07:31:31,192 >> loading file https://huggingface.co/gpt2-medium/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-12 07:31:31,192 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1394] 2021-04-12 07:31:31,818 >> https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7s_rqlih\n",
            "Downloading: 100% 1.52G/1.52G [00:34<00:00, 44.2MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-12 07:32:07,251 >> storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|file_utils.py:1401] 2021-04-12 07:32:07,252 >> creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1069] 2021-04-12 07:32:07,252 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1198] 2021-04-12 07:32:22,677 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1207] 2021-04-12 07:32:22,677 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 14/14 [00:00<00:00, 40.22ba/s]\n",
            "100% 14/14 [00:00<00:00, 18.32ba/s]\n",
            "[INFO|trainer.py:1011] 2021-04-12 07:32:33,862 >> ***** Running training *****\n",
            "[INFO|trainer.py:1012] 2021-04-12 07:32:33,862 >>   Num examples = 689\n",
            "[INFO|trainer.py:1013] 2021-04-12 07:32:33,862 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1014] 2021-04-12 07:32:33,862 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1015] 2021-04-12 07:32:33,862 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1016] 2021-04-12 07:32:33,862 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1017] 2021-04-12 07:32:33,863 >>   Total optimization steps = 1378\n",
            "{'loss': 2.6571, 'learning_rate': 3.18577648766328e-05, 'epoch': 0.73}\n",
            "{'loss': 2.0777, 'learning_rate': 1.3715529753265602e-05, 'epoch': 1.45}\n",
            "100% 1378/1378 [15:36<00:00,  1.48it/s][INFO|trainer.py:1198] 2021-04-12 07:48:10,569 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 936.707, 'train_samples_per_second': 1.471, 'epoch': 2.0}\n",
            "100% 1378/1378 [15:36<00:00,  1.47it/s]\n",
            "[INFO|trainer.py:1667] 2021-04-12 07:48:10,825 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:329] 2021-04-12 07:48:10,825 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-12 07:48:15,970 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1907] 2021-04-12 07:48:15,971 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1913] 2021-04-12 07:48:15,971 >> Special tokens file saved in output/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:722] 2021-04-12 07:48:16,051 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   epoch                      =        2.0\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   init_mem_cpu_alloc_delta   =      272MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   init_mem_gpu_alloc_delta   =     1377MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_mem_cpu_alloc_delta  =       10MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_mem_gpu_alloc_delta  =     4061MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_mem_gpu_peaked_delta =      977MB\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_runtime              = 0:15:36.70\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_samples              =        689\n",
            "[INFO|trainer_pt_utils.py:727] 2021-04-12 07:48:16,052 >>   train_samples_per_second   =      1.471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObITJiAq_Hh9",
        "outputId": "8c1480c6-3603-4303-8800-476bfb613129"
      },
      "source": [
        "import subprocess\n",
        "user_prompt = input(\"Enter the prompt for which the screenplay is to be generated: \")\n",
        "\n",
        "output = subprocess.Popen(f\"python transformers/examples/text-generation/run_generation.py \"\n",
        "    \"--model_type gpt2 \"\n",
        "    \"--model_name_or_path output \"\n",
        "    \"--length 1000 \"\n",
        "    f\"--prompt \\\"{user_prompt}\\\" \"\n",
        "    \"> test.txt\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "std_out, std_err = output.communicate()\n",
        "\n",
        "print(\"OUTPUT:\", std_out.decode())\n",
        "print(\"ERROR:\", std_err.decode())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the prompt for which the screenplay is to be generated: The T-rex let out a huge roar\n",
            "OUTPUT: \n",
            "ERROR: 2021-04-12 07:48:39.733239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/12/2021 07:48:41 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False\n",
            "04/12/2021 07:48:58 - INFO - __main__ -   Namespace(device=device(type='cuda'), fp16=False, k=0, length=1000, model_name_or_path='output', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prefix='', prompt='The T-rex let out a huge roar', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx9YwYXJwFAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1877d0e7-1440-45ae-dcca-2e90a199a572"
      },
      "source": [
        "\n",
        "names = [\"Alan\", \"Grant\", \"Ellie\", \"Ian\", \"John\", \"Hammond\", \"Robert\",\"Muldoon\", \"Donald\", \"Gennaro\",\n",
        " \"Henry\", \"Timmy\", \"Lex\", \"Arnold\", \"Ray\", \"Dennis\", \"Nedry\", \"Harding\", \"Lewis\",\"Dodgson\",\n",
        "  \"Mr. DNA\", \"Ed\", \"Regis\", \"Marty\", \"Sarah\", \"Harding\", \"Roland\",\"Tembo\", \"Peter\", \"Ludlow\", \"Nick\", \n",
        "  \"Kelly\", \"Curtis\",\"Dieter\", \"Stark\",\"Ajay\",\"Eddie\",\"Robert\",\"Richard\",\n",
        "  \"Jack\",\"Arby\", \"Benton\", \"Howard\", \"George\", \"Baselton\",\"Diego\", \"Paul\", \"Kirby\", \"Amanda\",\n",
        "   \"Billy\", \"Brennan\", \"Eric\", \"Kirby\", \"Udesky\", \"Cooper\", \"Nash\", \"Mark\", \"Ben\", \"Hildebrand\", \"Enrique\", \"Cardoso\", \"Charlie\"]\n",
        "\n",
        "names = [i.upper() for i in names]\n",
        "print(names)\n",
        "\n",
        "with open(\"test.txt\") as f1, open(\"formatted.txt\", \"w\") as f2:\n",
        "    lines = f1.readlines()\n",
        "    newlines = [] \n",
        "    for i in lines:\n",
        "        # print(re.sub(r\"\\W([A-Z]{2,})\\W\", r\"\\1:\\n\", i))\n",
        "        a = re.sub(r\"(\" + r\"|\".join(names) + r\")\", r\"\\n\\1\", i)\n",
        "        # a = re.sub(r\"GENERATED\", r\"\\nBRUH: \", i)\n",
        "        newlines.append(a)\n",
        "        # print(\"Found in\", a)\n",
        "\n",
        "    # print(lines)\n",
        "    f2.writelines(newlines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ALAN', 'GRANT', 'ELLIE', 'IAN', 'JOHN', 'HAMMOND', 'ROBERT', 'MULDOON', 'DONALD', 'GENNARO', 'HENRY', 'TIMMY', 'LEX', 'ARNOLD', 'RAY', 'DENNIS', 'NEDRY', 'HARDING', 'LEWIS', 'DODGSON', 'MR. DNA', 'ED', 'REGIS', 'MARTY', 'SARAH', 'HARDING', 'ROLAND', 'TEMBO', 'PETER', 'LUDLOW', 'NICK', 'KELLY', 'CURTIS', 'DIETER', 'STARK', 'AJAY', 'EDDIE', 'ROBERT', 'RICHARD', 'JACK', 'ARBY', 'BENTON', 'HOWARD', 'GEORGE', 'BASELTON', 'DIEGO', 'PAUL', 'KIRBY', 'AMANDA', 'BILLY', 'BRENNAN', 'ERIC', 'KIRBY', 'UDESKY', 'COOPER', 'NASH', 'MARK', 'BEN', 'HILDEBRAND', 'ENRIQUE', 'CARDOSO', 'CHARLIE']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}